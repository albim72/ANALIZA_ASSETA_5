# -*- coding: utf-8 -*-
"""analiza_tesktu_recenzje_filmy_full(1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_PMVQMxkD-7tmCS650aNiD8beDcV7VBy
"""

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.keras import losses
import os
import shutil
import re

url = "https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
dataset = tf.keras.utils.get_file("aclImdb_v1",url,untar=True,cache_dir='.',cache_subdir='')

dataset_dir = os.path.join(os.path.dirname(dataset),'aclImdb_v1/aclImdb')

os.listdir(dataset_dir)

train_dir = os.path.join(dataset_dir,'train')
os.listdir(train_dir)

remove_dir = os.path.join(train_dir,'unsup')
shutil.rmtree(remove_dir)

os.listdir(train_dir)

sample_file = os.path.join(train_dir,'pos/1181_9.txt')
with open(sample_file) as f:
    print(f.read())

batch_size = 32
seed = 42

raw_train_ds = tf.keras.utils.text_dataset_from_directory(
    'aclImdb_v1/aclImdb/train',
    batch_size=batch_size,
    validation_split=0.2,
    subset='training',
    seed=seed
)

for text_batch, label_batch in raw_train_ds.take(1):
    for i in range(5):
        print("Recenzja",text_batch.numpy()[i])
        print("Etykieta",label_batch.numpy()[i])

print("Liczba etykiet",len(raw_train_ds.class_names))
print("Etykieta 0:",raw_train_ds.class_names[0])
print("Etykieta 1:",raw_train_ds.class_names[1])

raw_val_ds = tf.keras.utils.text_dataset_from_directory(
    'aclImdb_v1/aclImdb/train',
    batch_size=batch_size,
    validation_split=0.2,
    subset='validation',
    seed=seed
)

raw_test_ds = tf.keras.utils.text_dataset_from_directory(
    'aclImdb_v1/aclImdb/test',
    batch_size=batch_size
)

import string

#procesy normalizacji danych
#standardyzacja danych tekstowych
def custom_standardization(input_data):
    lowercase = tf.strings.lower(input_data)
    stripped_html = tf.strings.regex_replace(lowercase,'<br />',' ')
    return tf.strings.regex_replace(stripped_html,'[%s]' % re.escape(string.punctuation),'')

#wektoryzacja tekstów
max_features = 10_000
sequence_length = 300
vectorize_layer = layers.TextVectorization(
    standardize=custom_standardization,
    max_tokens=max_features,
    output_mode='int',
    output_sequence_length=sequence_length
)

train_text = raw_train_ds.map(lambda x,y:x)
vectorize_layer.adapt(train_text)

def vectorize_text(text,label):
    text = tf.expand_dims(text,-1)
    return vectorize_layer(text),label

text_batch, label_batch = next(iter(raw_train_ds))
first_review, first_label = text_batch[0],label_batch[0]
print("Recenzja:",first_review)
print("Etykieta:",raw_train_ds.class_names[first_label])
print("Widok zwektoryzowany tesktu recenzji:",vectorize_text(first_review,first_label))

print("Rozmiar słownika:",vectorize_layer.vocabulary_size())
print(f"15 --> {vectorize_layer.get_vocabulary()[15]}")
print(f"618 --> {vectorize_layer.get_vocabulary()[618]}")
print(f"313 --> {vectorize_layer.get_vocabulary()[313]}")
print(f"7 --> {vectorize_layer.get_vocabulary()[7]}")
print(f"11 --> {vectorize_layer.get_vocabulary()[11]}")
print(f"1287 --> {vectorize_layer.get_vocabulary()[1287]}")
print(f"7080 --> {vectorize_layer.get_vocabulary()[7080]}")

train_ds = raw_train_ds.map(vectorize_text)
val_ds = raw_val_ds.map(vectorize_text)
test_ds = raw_test_ds.map(vectorize_text)

AUTOTUNE = tf.data.AUTOTUNE

train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)
test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)

#tworzenie modelu
embedding_dim = 16
model = tf.keras.Sequential([
    layers.Embedding(max_features+1,embedding_dim),
    layers.Dropout(0.2),
    layers.GlobalAveragePooling1D(),
    layers.Dropout(0.2),
    layers.Dense(1)
])

model.summary()

model.compile(loss=losses.BinaryCrossentropy(from_logits=True),
              optimizer='adam',
              metrics=[tf.metrics.BinaryAccuracy(threshold=0.01)])

epochs = 10
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=epochs
)

loss, accuracy = model.evaluate(test_ds)

print("Loss:",loss)
print("Accuracy:",accuracy)

history_dict = history.history
history_dict.keys()

acc = history_dict['binary_accuracy']
val_acc = history_dict['val_binary_accuracy']
loss = history_dict['loss']
val_loss = history_dict['val_loss']

epochs = range(1,len(acc)+1)
plt.plot(epochs,loss,'bo',label='Training loss')
plt.plot(epochs,val_loss,'b',label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt

export_model = tf.keras.Sequential([
    vectorize_layer,
    model,
    layers.Activation('sigmoid')
])

export_model.compile(
    loss=losses.BinaryCrossentropy(from_logits=False),
    optimizer='adam',
    metrics=['accuracy']
)

metrics = export_model.evaluate(raw_test_ds)
print(metrics)
loss = metrics[0]
accuracy = metrics[1]
print(f"Loss: {loss}")
print(f"Accuracy: {accuracy}")
print(f"Real Accuracy: {accuracy*100:.2f}%")

testy = [
    "The movie was great! And very good! Excellent! Bingo!",
    "The movie was okay.",
    "The movie was terrible.",
    "Very bad movie!",
    "Big Shit!",
    "Disaster.",
    "Beautiful disaster",
    "Głupi film.",
    "Głupie filmidło.",
    "The movie was bad bad bad bad bad bad",
    "The movie was okay okay okay okay okay okay okay okay.",
    "Great Great Great Great Great Great Great",
    "The movie was good with good acting but a poor screenplay.",
    "The movie is very very good good good good good good good!",
    "Good!",
    "very very good good good",
    "film was very decent but visually stunning!"


]

testy_tensor = tf.convert_to_tensor(testy)
export_model.predict(testy_tensor)